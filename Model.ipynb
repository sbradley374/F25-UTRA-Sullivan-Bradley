{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 384, 384])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.Tensor(np.random.random((4,3,384,384)))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_ignore(nn.Module):\n",
    "    def __init__(self, start_index=1):\n",
    "        super(Read_ignore, self).__init__()\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, self.start_index:]\n",
    "\n",
    "\n",
    "class Read_add(nn.Module):\n",
    "    def __init__(self, start_index=1):\n",
    "        super(Read_add, self).__init__()\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.start_index == 2:\n",
    "            readout = (x[:, 0] + x[:, 1]) / 2\n",
    "        else:\n",
    "            readout = x[:, 0]\n",
    "        return x[:, self.start_index :] + readout.unsqueeze(1)\n",
    "\n",
    "\n",
    "class Read_projection(nn.Module):\n",
    "    def __init__(self, in_features, start_index=1):\n",
    "        super(Read_projection, self).__init__()\n",
    "        self.start_index = start_index\n",
    "        self.project = nn.Sequential(nn.Linear(2 * in_features, in_features), nn.GELU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index :])\n",
    "        features = torch.cat((x[:, self.start_index :], readout), -1)\n",
    "        return self.project(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resample(nn.Module):\n",
    "    def __init__(self, p, s, emb_dim, resample_dim):\n",
    "        super(Resample, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(emb_dim,\n",
    "                               resample_dim, \n",
    "                               kernel_size=1,\n",
    "                               stride=1,\n",
    "                               padding=0)\n",
    "        self.conv2 = None\n",
    "        if s >= p:\n",
    "            self.conv2 = nn.Conv2d(resample_dim,\n",
    "                                   resample_dim, \n",
    "                                   kernel_size=3,\n",
    "                                   stride=3,\n",
    "                                   padding=0,\n",
    "                                   bias=True)\n",
    "        else:\n",
    "            self.conv2 = nn.ConvTranspose2d(resample_dim, \n",
    "                                            resample_dim, \n",
    "                                            kernel_size=3, \n",
    "                                            stride=3,\n",
    "                                            padding=0,\n",
    "                                            bias=True,\n",
    "                                            dilation=1,\n",
    "                                            groups=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reassemble(nn.Module):\n",
    "    def __init__(self, image_size, read, p, s, emb_dim, resample_dim):\n",
    "        \"\"\"\n",
    "        p = patch size \n",
    "        s = coefficient resample\n",
    "        emb_dim <=> D (in the paper)\n",
    "        resample_dim <=> ^D (in the paper)\n",
    "        read : {\"ignore\", \"add\", \"projection\"}\n",
    "        \"\"\"\n",
    "        super(Reassemble, self).__init__()\n",
    "        channels, image_height, image_width = image_size\n",
    "        \n",
    "        #Read\n",
    "        self.read = Read_ignore()\n",
    "        if read == 'add':\n",
    "            self.read = Read_add()\n",
    "        elif read == 'projection':\n",
    "            self.read = Read_projection(emb_dim)\n",
    "    \n",
    "        #Concat after read\n",
    "        self.concat = Rearrange('b (h w) c -> b c h w', \n",
    "                                c=emb_dim,\n",
    "                                h=(image_height // p),\n",
    "                                w=(image_width // p))\n",
    "        \n",
    "        #Projection + Resample\n",
    "        self.resample = Resample(p, s, emb_dim, resample_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = self.read(x)\n",
    "        print(x.shape)\n",
    "        x = self.concat(x)\n",
    "        print(x.shape)\n",
    "        x = self.resample(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocusOnDepth(nn.Module):\n",
    "    def __init__(self, \n",
    "                 image_size = (3, 384, 384), \n",
    "                 patch_size = 16, \n",
    "                 emb_dim = 1024,\n",
    "                 resample_dim = 256,\n",
    "                 read = 'projection',\n",
    "                 num_layers_encoder = 24,\n",
    "                 hooks = [5, 11, 17, 23],\n",
    "                 reassemble_s = [4, 8, 16, 32],\n",
    "                 nhead = 16,\n",
    "                 transformer_dropout = 0):\n",
    "        \"\"\"\n",
    "        Focus on Depth - Large\n",
    "        image_size : (c, h, w)\n",
    "        patch_size : *a square*\n",
    "        emb_dim <=> D (in the paper)\n",
    "        resample_dim <=> ^D (in the paper)\n",
    "        read : {\"ignore\", \"add\", \"projection\"}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Splitting img into patches\n",
    "        channels, image_height, image_width = image_size\n",
    "        assert image_height % patch_size == 0 and image_width % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_height // patch_size) * (image_width // patch_size)\n",
    "        patch_dim = channels * patch_size * patch_size\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_dim, emb_dim),\n",
    "        )\n",
    "        #Embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
    "        \n",
    "        #Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dropout=transformer_dropout, dim_feedforward=emb_dim*4)\n",
    "        self.transformer_encoders = nn.TransformerEncoder(encoder_layer, num_layers=num_layers_encoder)\n",
    "        #Register hooks\n",
    "        self.activation = {}\n",
    "        self.hooks = hooks\n",
    "        self._get_layers_from_hooks(self.hooks)\n",
    "        \n",
    "        #Reassembles\n",
    "        self.reassembles = []\n",
    "        for s in reassemble_s:\n",
    "            self.reassembles.append(Reassemble(image_size, read, patch_size, s, emb_dim, resample_dim))\n",
    "    \n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        t = self.transformer_encoders(x)\n",
    "        t0 = self.activation['t5']\n",
    "        t0 = self.reassembles[0](t0)\n",
    "\n",
    "    def _get_layers_from_hooks(self, hooks):\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activation[name] = output.detach()\n",
    "            return hook\n",
    "        for h in hooks:\n",
    "            self.transformer_encoders.layers[h].register_forward_hook(get_activation('t'+str(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 577, 128])\n",
      "torch.Size([4, 576, 128])\n",
      "torch.Size([4, 128, 24, 24])\n",
      "torch.Size([4, 256, 72, 72])\n"
     ]
    }
   ],
   "source": [
    "model = FocusOnDepth((3,384,384), patch_size=16, emb_dim=128, read=\"projection\")\n",
    "model(img)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
