{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 384, 384])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.Tensor(np.random.random((4,3,384,384)))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_ignore(nn.Module):\n",
    "    def __init__(self, start_index=1):\n",
    "        super(Read_ignore, self).__init__()\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, self.start_index:]\n",
    "\n",
    "\n",
    "class Read_add(nn.Module):\n",
    "    def __init__(self, start_index=1):\n",
    "        super(Read_add, self).__init__()\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.start_index == 2:\n",
    "            readout = (x[:, 0] + x[:, 1]) / 2\n",
    "        else:\n",
    "            readout = x[:, 0]\n",
    "        return x[:, self.start_index :] + readout.unsqueeze(1)\n",
    "\n",
    "\n",
    "class Read_projection(nn.Module):\n",
    "    def __init__(self, in_features, start_index=1):\n",
    "        super(Read_projection, self).__init__()\n",
    "        self.start_index = start_index\n",
    "        self.project = nn.Sequential(nn.Linear(2 * in_features, in_features), nn.GELU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index :])\n",
    "        features = torch.cat((x[:, self.start_index :], readout), -1)\n",
    "        return self.project(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocusOnDepth(nn.Module):\n",
    "    def __init__(self, \n",
    "                 image_size = (3, 384, 384), \n",
    "                 patch_size = (16, 16), \n",
    "                 emb_dim = 1024, \n",
    "                 read = 'projection',\n",
    "                 num_layers_encoder = 24,\n",
    "                 hooks = [0,1,2,3],\n",
    "                 nhead = 16,\n",
    "                 transformer_dropout = 0):\n",
    "        \"\"\"\n",
    "        Focus on Depth - Large\n",
    "        image_size : (c, h, w)\n",
    "        patch_size : (h, w)\n",
    "        emb_dim <=> D (in the paper)\n",
    "        read : {\"ignore\", \"add\", \"projection\"}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Splitting img into patches\n",
    "        channels, image_height, image_width = image_size\n",
    "        patch_height, patch_width = patch_size\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_height // patch_height) * \\\n",
    "                      (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, emb_dim),\n",
    "        )\n",
    "        #Embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
    "        \n",
    "        #Transformer\n",
    "        self.activation = {}\n",
    "        self.hooks = hooks\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dropout=transformer_dropout, dim_feedforward=emb_dim*4)\n",
    "        self.transformer_encoders = nn.TransformerEncoder(encoder_layer, num_layers=num_layers_encoder)\n",
    "        #Register hooks\n",
    "        self.get_layers_from_hooks(self.hooks)\n",
    "\n",
    "        #Concat after read\n",
    "        self.concat = nn.Sequential(\n",
    "            Rearrange('b (h w) c -> b c h w', c=emb_dim,  h=(image_height // patch_height), w=(image_width // patch_width))\n",
    "        )\n",
    "\n",
    "        #Read\n",
    "        self.read = Read_ignore()\n",
    "        if read == 'add':\n",
    "            self.read = Read_add()\n",
    "        elif read == 'projection':\n",
    "            self.read = Read_projection(emb_dim)\n",
    "    \n",
    "    def get_layers_from_hooks(self, hooks):\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activation[name] = output.detach()\n",
    "            return hook\n",
    "        for h in hooks:\n",
    "            self.transformer_encoders.layers[h].register_forward_hook(get_activation('t'+str(h)))\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        print(x.shape)\n",
    "        t = self.transformer_encoders(x)\n",
    "        print(t.shape)\n",
    "        act1 = self.activation['t1']\n",
    "        act1 = self.read(act1)\n",
    "        act1 = self.concat(act1)\n",
    "        print(act1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 577, 128])\n",
      "torch.Size([4, 577, 128])\n",
      "torch.Size([4, 128, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "model = FocusOnDepth((3,384,384), (16, 16), 128, \"projection\", 5, [1], 4)\n",
    "model(img)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t1': tensor([[[ 0.5244,  0.6776,  0.9152,  ..., -1.4480,  0.3267, -2.3474],\n",
       "          [ 1.3049, -1.7115, -0.2993,  ..., -1.0103, -0.7556,  0.3068],\n",
       "          [-0.5465,  0.7846, -0.2911,  ...,  0.8926,  1.2563,  1.3280],\n",
       "          ...,\n",
       "          [ 1.3120, -1.8247, -0.4216,  ...,  1.1285,  0.2768,  1.0452],\n",
       "          [ 0.1289,  1.7938, -0.1779,  ..., -0.4079,  0.5089,  0.8926],\n",
       "          [ 0.3298,  1.3368, -0.1826,  ...,  1.6811, -1.1189,  0.5914]],\n",
       " \n",
       "         [[ 0.5244,  0.6776,  0.9152,  ..., -1.4480,  0.3267, -2.3474],\n",
       "          [ 1.4871, -1.9232, -0.0783,  ..., -0.9625, -0.7430,  0.1225],\n",
       "          [-0.0629,  0.7538, -0.5719,  ...,  0.8150,  1.4043,  1.3633],\n",
       "          ...,\n",
       "          [ 1.3689, -1.7542, -0.3998,  ...,  1.2487,  0.3106,  1.0138],\n",
       "          [ 0.1369,  1.4279, -0.2067,  ..., -0.1465,  0.4153,  0.9271],\n",
       "          [ 0.2446,  1.2247, -0.1281,  ...,  1.7195, -1.1244,  0.5852]],\n",
       " \n",
       "         [[ 0.5244,  0.6776,  0.9152,  ..., -1.4480,  0.3267, -2.3474],\n",
       "          [ 1.1764, -1.8925, -0.3179,  ..., -1.1715, -0.5165,  0.0213],\n",
       "          [-0.2667,  0.8398, -0.5892,  ...,  0.7896,  1.2483,  1.0850],\n",
       "          ...,\n",
       "          [ 1.4595, -1.6214, -0.5507,  ...,  1.2181,  0.3336,  0.9286],\n",
       "          [-0.0252,  1.6496, -0.2187,  ..., -0.2552,  0.5039,  0.9969],\n",
       "          [ 0.3149,  1.4336, -0.0864,  ...,  1.9062, -1.1672,  0.5261]],\n",
       " \n",
       "         [[ 0.5244,  0.6776,  0.9152,  ..., -1.4480,  0.3267, -2.3474],\n",
       "          [ 1.0086, -1.7718, -0.4751,  ..., -0.9366, -0.4787,  0.2037],\n",
       "          [-0.4165,  0.5058, -0.3751,  ...,  0.9457,  1.0893,  1.3572],\n",
       "          ...,\n",
       "          [ 1.3200, -1.4934, -0.6153,  ...,  1.1780,  0.2410,  1.0892],\n",
       "          [ 0.1085,  1.6669, -0.2288,  ..., -0.2005,  0.5353,  0.8761],\n",
       "          [ 0.3006,  1.4438,  0.0654,  ...,  1.8532, -1.0942,  0.6570]]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=1024, nhead=16, dropout=0, dim_feedforward=1024*4)\n",
    "transformer_encoders = nn.TransformerEncoder(encoder_layer, num_layers=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (linear2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0, inplace=False)\n",
       "  (dropout2): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
