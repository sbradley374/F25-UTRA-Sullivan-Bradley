{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import timm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 384, 384])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.Tensor(np.random.random((4,3,384,384)))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_ignore(nn.Module):\n",
    "    def __init__(self, start_index=1):\n",
    "        super(Read_ignore, self).__init__()\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, self.start_index:]\n",
    "\n",
    "\n",
    "class Read_add(nn.Module):\n",
    "    def __init__(self, start_index=1):\n",
    "        super(Read_add, self).__init__()\n",
    "        self.start_index = start_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.start_index == 2:\n",
    "            readout = (x[:, 0] + x[:, 1]) / 2\n",
    "        else:\n",
    "            readout = x[:, 0]\n",
    "        return x[:, self.start_index :] + readout.unsqueeze(1)\n",
    "\n",
    "\n",
    "class Read_projection(nn.Module):\n",
    "    def __init__(self, in_features, start_index=1):\n",
    "        super(Read_projection, self).__init__()\n",
    "        self.start_index = start_index\n",
    "        self.project = nn.Sequential(nn.Linear(2 * in_features, in_features), nn.GELU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        readout = x[:, 0].unsqueeze(1).expand_as(x[:, self.start_index :])\n",
    "        features = torch.cat((x[:, self.start_index :], readout), -1)\n",
    "        return self.project(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConvTranspose2d(nn.Module):\n",
    "    def __init__(self, conv, output_size):\n",
    "        super(MyConvTranspose2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.conv = conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x, output_size=self.output_size)\n",
    "        return x\n",
    "\n",
    "class Resample(nn.Module):\n",
    "    def __init__(self, p, s, h, emb_dim, resample_dim):\n",
    "        super(Resample, self).__init__()\n",
    "        assert (s in [4, 8, 16, 32]), \"s must be in [0.5, 4, 8, 16, 32]\"\n",
    "        self.conv1 = nn.Conv2d(emb_dim, resample_dim, kernel_size=1, stride=1, padding=0)\n",
    "        if s == 4:\n",
    "            self.conv2 = nn.ConvTranspose2d(resample_dim, \n",
    "                                resample_dim, \n",
    "                                kernel_size=4, \n",
    "                                stride=4,\n",
    "                                padding=0,\n",
    "                                bias=True,\n",
    "                                dilation=1,\n",
    "                                groups=1)\n",
    "        elif s == 8:\n",
    "            self.conv2 = nn.ConvTranspose2d(resample_dim, \n",
    "                                resample_dim, \n",
    "                                kernel_size=2, \n",
    "                                stride=2,\n",
    "                                padding=0,\n",
    "                                bias=True,\n",
    "                                dilation=1,\n",
    "                                groups=1)\n",
    "        elif s == 16:\n",
    "            self.conv2 = nn.Identity()\n",
    "        else:\n",
    "            self.conv2 = nn.Conv2d(resample_dim, resample_dim, kernel_size=2,stride=2, padding=0, bias=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class ResidualConvUnit(nn.Module):\n",
    "\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            features, features, kernel_size=3, stride=1, padding=1, bias=True\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            features, features, kernel_size=3, stride=1, padding=1, bias=True\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            x (tensor): input\n",
    "        Returns:\n",
    "            tensor: output\n",
    "        \"\"\"\n",
    "        out = self.relu(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out + x\n",
    "\n",
    "class Fusion(nn.Module):\n",
    "    def __init__(self, resample_dim):\n",
    "        super(Fusion, self).__init__()\n",
    "        self.res_conv1 = ResidualConvUnit(resample_dim)\n",
    "        self.res_conv2 = ResidualConvUnit(resample_dim)\n",
    "        #self.resample = nn.ConvTranspose2d(resample_dim, resample_dim, kernel_size=2, stride=2, padding=0, bias=True, dilation=1, groups=1)\n",
    "    \n",
    "    def forward(self, x, previous_stage=None):\n",
    "        if not previous_stage:\n",
    "            previous_stage = torch.zeros_like(x)\n",
    "        \n",
    "        output_stage1 = self.res_conv1(x)\n",
    "        output_stage1 += previous_stage\n",
    "\n",
    "        output_stage2 = self.res_conv2(output_stage1)\n",
    "        output_stage2 = nn.functional.interpolate(\n",
    "            output_stage2, scale_factor=2, mode=\"bilinear\", align_corners=True\n",
    "        )\n",
    "        return output_stage2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reassemble(nn.Module):\n",
    "    def __init__(self, image_size, read, p, s, emb_dim, resample_dim):\n",
    "        \"\"\"\n",
    "        p = patch size \n",
    "        s = coefficient resample\n",
    "        emb_dim <=> D (in the paper)\n",
    "        resample_dim <=> ^D (in the paper)\n",
    "        read : {\"ignore\", \"add\", \"projection\"}\n",
    "        \"\"\"\n",
    "        super(Reassemble, self).__init__()\n",
    "        channels, image_height, image_width = image_size\n",
    "        \n",
    "        #Read\n",
    "        self.read = Read_ignore()\n",
    "        if read == 'add':\n",
    "            self.read = Read_add()\n",
    "        elif read == 'projection':\n",
    "            self.read = Read_projection(emb_dim)\n",
    "    \n",
    "        #Concat after read\n",
    "        self.concat = Rearrange('b (h w) c -> b c h w', \n",
    "                                c=emb_dim,\n",
    "                                h=(image_height // p),\n",
    "                                w=(image_width // p))\n",
    "        \n",
    "        #Projection + Resample\n",
    "        self.resample = Resample(p, s, image_height, emb_dim, resample_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.read(x)\n",
    "        x = self.concat(x)\n",
    "        x = self.resample(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocusOnDepth(nn.Module):\n",
    "    def __init__(self, \n",
    "                 image_size = (3, 384, 384), \n",
    "                 patch_size = 16, \n",
    "                 emb_dim = 1024,\n",
    "                 resample_dim = 256,\n",
    "                 read = 'projection',\n",
    "                 num_layers_encoder = 24,\n",
    "                 hooks = [5, 11, 17, 23],\n",
    "                 reassemble_s = [4, 8, 16, 32],\n",
    "                 nhead = 16,\n",
    "                 transformer_dropout = 0):\n",
    "        \"\"\"\n",
    "        Focus on Depth - Large\n",
    "        image_size : (c, h, w)\n",
    "        patch_size : *a square*\n",
    "        emb_dim <=> D (in the paper)\n",
    "        resample_dim <=> ^D (in the paper)\n",
    "        read : {\"ignore\", \"add\", \"projection\"}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Splitting img into patches\n",
    "        channels, image_height, image_width = image_size\n",
    "        assert image_height % patch_size == 0 and image_width % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_height // patch_size) * (image_width // patch_size)\n",
    "        patch_dim = channels * patch_size * patch_size\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_dim, emb_dim),\n",
    "        )\n",
    "        #Embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
    "        \n",
    "        #Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead, dropout=transformer_dropout, dim_feedforward=emb_dim*4)\n",
    "        self.transformer_encoders = nn.TransformerEncoder(encoder_layer, num_layers=num_layers_encoder)\n",
    "        #Register hooks\n",
    "        self.activation = {}\n",
    "        self.hooks = hooks\n",
    "        self._get_layers_from_hooks(self.hooks)\n",
    "        \n",
    "        #Reassembles Fusion\n",
    "        self.reassembles = []\n",
    "        self.fusions = []\n",
    "        for s in reassemble_s:\n",
    "            self.reassembles.append(Reassemble(image_size, read, patch_size, s, emb_dim, resample_dim))\n",
    "            self.fusions.append(Fusion(resample_dim))\n",
    "        self.reassembles = nn.ModuleList(self.reassembles)\n",
    "        self.fusions = nn.ModuleList(self.fusions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        t = self.transformer_encoders(x)\n",
    "        previous_stage = None\n",
    "        for i in np.arange(len(self.fusions)-1, -1, -1):\n",
    "            hook_to_take = 't'+str(self.hooks[i])\n",
    "            activation_result = self.activation[hook_to_take]\n",
    "            reassemble_result = self.reassembles[i](activation_result)\n",
    "            fusion_result = self.fusions[i](reassemble_result, previous_stage)\n",
    "            previous_stage = fusion_result\n",
    "            print(fusion_result.shape)\n",
    "\n",
    "\n",
    "        #for i,(_,v) in enumerate(self.activation.items()):\n",
    "        #    self.reassembles[i](v)\n",
    "\n",
    "    def _get_layers_from_hooks(self, hooks):\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                self.activation[name] = output.detach()\n",
    "            return hook\n",
    "        for h in hooks:\n",
    "            self.transformer_encoders.layers[h].register_forward_hook(get_activation('t'+str(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6519/1904494174.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFocusOnDepth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"projection\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6519/3709615186.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, image_size, patch_size, emb_dim, resample_dim, read, num_layers_encoder, hooks, reassemble_s, nhead, transformer_dropout)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreassemble_s\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreassembles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReassemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresample_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreassembles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreassembles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6519/1674824761.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, resample_dim)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m#self.resample = nn.ConvTranspose2d(resample_dim, resample_dim, kernel_size=2, stride=2, padding=0, bias=True, dilation=1, groups=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         self.resample = nn.functional.interpolate(\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bilinear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "model = FocusOnDepth((3,384,384), patch_size=16, emb_dim=128, read=\"projection\")\n",
    "model(img)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t5', 't11', 't17', 't23'])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16384, 27])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 128, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         ...,\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.],\n",
       "         [2., 2., 2.,  ..., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation des rearrange\n",
    "tmp = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "tmp = np.concatenate([np.concatenate([tmp] * 128, axis=1)]*128)\n",
    "tmp = np.stack([tmp,tmp,tmp], axis=0)\n",
    "tmp = np.array([tmp])\n",
    "tmp = torch.Tensor(tmp)\n",
    "tmp.shape\n",
    "\n",
    "topatch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=3, p2=3)\n",
    "tmp2 = topatch(tmp)\n",
    "tmp2 = torch.Tensor(tmp2)\n",
    "print(tmp2.shape)\n",
    "plt.imshow(tmp2[0], aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "concat = Rearrange('b (h w) c -> b c h w', c=27, h=(384 // 3), w=(384 // 3))\n",
    "tmp3 = concat(tmp2)\n",
    "tmp3 = torch.Tensor(tmp3)\n",
    "print(tmp3.shape)\n",
    "tmp3[0,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample2 = Resample(16, 2, 768, 768)\n",
    "tmp4 = resample2(tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 24, 24])\n",
      "torch.Size([1, 768, 72, 72])\n"
     ]
    }
   ],
   "source": [
    "print(tmp3.shape)\n",
    "print(tmp4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2 = nn.ConvTranspose2d(768, 768, \n",
    "                        kernel_size=3, \n",
    "                        stride=3,\n",
    "                        padding=0,\n",
    "                        bias=True,\n",
    "                        dilation=1,\n",
    "                        groups=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 49, 49])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp5 = conv2(tmp3)\n",
    "tmp5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
